{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Classification on Titanic Dataset\n",
    "\n",
    "We are going to use a dataset from a Kaggle competition (https://www.kaggle.com/c/titanic/data)\n",
    " \n",
    "### Dataset description\n",
    "\n",
    ">The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.  This tragedy shocked the international community and led to better safety regulations for ships.\n",
    "\n",
    ">One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.  Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
    "\n",
    ">In this contest, we ask you to complete the analysis of what sorts of people were more likely to survive. \n",
    "\n",
    "From the competition [homepage](http://www.kaggle.com/c/titanic-gettingStarted).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO: put your ID number (\"numero di matricola\")\n",
    "It will be used as seed for splitting the data into training and test. You can also try different seeds to see the impact of the random subdvision of the train and test sets and of the random components in the algorithm on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#put here your ``numero di matricola''\n",
    "IDnumber = 1211201 # substitute with your ID \n",
    "np.random.seed(IDnumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load library for plotting\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Load the data from a .csv file. In this notebook we use the pandas (Python Data Analysis Library) package, since it provides useful functions to clean the data. In particular, it allows us to remove samples with missing data, as we do below. We also plot some descriptions of columns, check the pandas documentation for 'describe()' if you want to know more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>712.000000</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>712.000000</td>\n",
       "      <td>712.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>448.589888</td>\n",
       "      <td>0.404494</td>\n",
       "      <td>2.240169</td>\n",
       "      <td>29.642093</td>\n",
       "      <td>0.514045</td>\n",
       "      <td>0.432584</td>\n",
       "      <td>34.567251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>258.683191</td>\n",
       "      <td>0.491139</td>\n",
       "      <td>0.836854</td>\n",
       "      <td>14.492933</td>\n",
       "      <td>0.930692</td>\n",
       "      <td>0.854181</td>\n",
       "      <td>52.938648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>222.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>445.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.645850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>677.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   712.000000  712.000000  712.000000  712.000000  712.000000   \n",
       "mean    448.589888    0.404494    2.240169   29.642093    0.514045   \n",
       "std     258.683191    0.491139    0.836854   14.492933    0.930692   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     222.750000    0.000000    1.000000   20.000000    0.000000   \n",
       "50%     445.000000    0.000000    2.000000   28.000000    0.000000   \n",
       "75%     677.250000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    5.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  712.000000  712.000000  \n",
       "mean     0.432584   34.567251  \n",
       "std      0.854181   52.938648  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    8.050000  \n",
       "50%      0.000000   15.645850  \n",
       "75%      1.000000   33.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's load pands and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# this time we use pandas to load and clean the dataset\n",
    "\n",
    "# read the data from the csv file\n",
    "df = pd.read_csv(\"data/titanicData.csv\")\n",
    "\n",
    "# remove columns 'Ticket', 'Cabin', and 'Name' from the data since they are not relevant\n",
    "df = df.drop(['Ticket','Cabin','Name'], axis=1)\n",
    "# remove samples with missing values\n",
    "df = df.dropna() \n",
    "# let's see some statistics about the data \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31.2750</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0292</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.5000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>31.3875</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>263.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.7208</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>82.1708</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>857</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>164.8667</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>858</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>859</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>19.2583</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>861</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14.1083</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>862</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>863</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.9292</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>865</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>866</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>867</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8583</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>868</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50.4958</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>870</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>871</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>872</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52.5542</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>873</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>874</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>875</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>876</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>877</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8458</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>878</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>83.1583</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>881</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>882</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>883</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5167</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>884</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>885</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0500</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>886</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>712 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass     Sex   Age  SibSp  Parch      Fare  \\\n",
       "0              1         0       3    male  22.0      1      0    7.2500   \n",
       "1              2         1       1  female  38.0      1      0   71.2833   \n",
       "2              3         1       3  female  26.0      0      0    7.9250   \n",
       "3              4         1       1  female  35.0      1      0   53.1000   \n",
       "4              5         0       3    male  35.0      0      0    8.0500   \n",
       "6              7         0       1    male  54.0      0      0   51.8625   \n",
       "7              8         0       3    male   2.0      3      1   21.0750   \n",
       "8              9         1       3  female  27.0      0      2   11.1333   \n",
       "9             10         1       2  female  14.0      1      0   30.0708   \n",
       "10            11         1       3  female   4.0      1      1   16.7000   \n",
       "11            12         1       1  female  58.0      0      0   26.5500   \n",
       "12            13         0       3    male  20.0      0      0    8.0500   \n",
       "13            14         0       3    male  39.0      1      5   31.2750   \n",
       "14            15         0       3  female  14.0      0      0    7.8542   \n",
       "15            16         1       2  female  55.0      0      0   16.0000   \n",
       "16            17         0       3    male   2.0      4      1   29.1250   \n",
       "18            19         0       3  female  31.0      1      0   18.0000   \n",
       "20            21         0       2    male  35.0      0      0   26.0000   \n",
       "21            22         1       2    male  34.0      0      0   13.0000   \n",
       "22            23         1       3  female  15.0      0      0    8.0292   \n",
       "23            24         1       1    male  28.0      0      0   35.5000   \n",
       "24            25         0       3  female   8.0      3      1   21.0750   \n",
       "25            26         1       3  female  38.0      1      5   31.3875   \n",
       "27            28         0       1    male  19.0      3      2  263.0000   \n",
       "30            31         0       1    male  40.0      0      0   27.7208   \n",
       "33            34         0       2    male  66.0      0      0   10.5000   \n",
       "34            35         0       1    male  28.0      1      0   82.1708   \n",
       "35            36         0       1    male  42.0      1      0   52.0000   \n",
       "37            38         0       3    male  21.0      0      0    8.0500   \n",
       "38            39         0       3  female  18.0      2      0   18.0000   \n",
       "..           ...       ...     ...     ...   ...    ...    ...       ...   \n",
       "856          857         1       1  female  45.0      1      1  164.8667   \n",
       "857          858         1       1    male  51.0      0      0   26.5500   \n",
       "858          859         1       3  female  24.0      0      3   19.2583   \n",
       "860          861         0       3    male  41.0      2      0   14.1083   \n",
       "861          862         0       2    male  21.0      1      0   11.5000   \n",
       "862          863         1       1  female  48.0      0      0   25.9292   \n",
       "864          865         0       2    male  24.0      0      0   13.0000   \n",
       "865          866         1       2  female  42.0      0      0   13.0000   \n",
       "866          867         1       2  female  27.0      1      0   13.8583   \n",
       "867          868         0       1    male  31.0      0      0   50.4958   \n",
       "869          870         1       3    male   4.0      1      1   11.1333   \n",
       "870          871         0       3    male  26.0      0      0    7.8958   \n",
       "871          872         1       1  female  47.0      1      1   52.5542   \n",
       "872          873         0       1    male  33.0      0      0    5.0000   \n",
       "873          874         0       3    male  47.0      0      0    9.0000   \n",
       "874          875         1       2  female  28.0      1      0   24.0000   \n",
       "875          876         1       3  female  15.0      0      0    7.2250   \n",
       "876          877         0       3    male  20.0      0      0    9.8458   \n",
       "877          878         0       3    male  19.0      0      0    7.8958   \n",
       "879          880         1       1  female  56.0      0      1   83.1583   \n",
       "880          881         1       2  female  25.0      0      1   26.0000   \n",
       "881          882         0       3    male  33.0      0      0    7.8958   \n",
       "882          883         0       3  female  22.0      0      0   10.5167   \n",
       "883          884         0       2    male  28.0      0      0   10.5000   \n",
       "884          885         0       3    male  25.0      0      0    7.0500   \n",
       "885          886         0       3  female  39.0      0      5   29.1250   \n",
       "886          887         0       2    male  27.0      0      0   13.0000   \n",
       "887          888         1       1  female  19.0      0      0   30.0000   \n",
       "889          890         1       1    male  26.0      0      0   30.0000   \n",
       "890          891         0       3    male  32.0      0      0    7.7500   \n",
       "\n",
       "    Embarked  \n",
       "0          S  \n",
       "1          C  \n",
       "2          S  \n",
       "3          S  \n",
       "4          S  \n",
       "6          S  \n",
       "7          S  \n",
       "8          S  \n",
       "9          C  \n",
       "10         S  \n",
       "11         S  \n",
       "12         S  \n",
       "13         S  \n",
       "14         S  \n",
       "15         S  \n",
       "16         Q  \n",
       "18         S  \n",
       "20         S  \n",
       "21         S  \n",
       "22         Q  \n",
       "23         S  \n",
       "24         S  \n",
       "25         S  \n",
       "27         S  \n",
       "30         C  \n",
       "33         S  \n",
       "34         C  \n",
       "35         S  \n",
       "37         S  \n",
       "38         S  \n",
       "..       ...  \n",
       "856        S  \n",
       "857        S  \n",
       "858        C  \n",
       "860        S  \n",
       "861        S  \n",
       "862        S  \n",
       "864        S  \n",
       "865        S  \n",
       "866        C  \n",
       "867        S  \n",
       "869        S  \n",
       "870        S  \n",
       "871        S  \n",
       "872        S  \n",
       "873        S  \n",
       "874        C  \n",
       "875        C  \n",
       "876        S  \n",
       "877        S  \n",
       "879        C  \n",
       "880        S  \n",
       "881        S  \n",
       "882        S  \n",
       "883        S  \n",
       "884        S  \n",
       "885        Q  \n",
       "886        S  \n",
       "887        S  \n",
       "889        C  \n",
       "890        Q  \n",
       "\n",
       "[712 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we create data matrices: many of the features (columns of indices 0,1,3,4,6 in Xcat below) are categorical, so we need to encode them with ***indicator matrices***. That is, if a feature can take $\\ell$ different values $v_1,\\dots,v_{\\ell}$, we create $\\ell$ indicator (0-1) features $I_1,\\dots,I_{\\ell}$, such that $I_{j} = 1$ if and only if the value of the feature is $v_j$. This can be done in Python by first encode a feature with integers with LabelEncoder() and then obtain the indicator variables with OneHotEncoder()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId', 'Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "#df.values contains the data, both the values of instances and the value of the label\n",
    "Data = df.values\n",
    "# the matrix including the categorical data is given by columns from the second one \n",
    "X_categorical = Data[:,2:]\n",
    "# the target value (class) is in the first column\n",
    "Y = Data[:,1]\n",
    "\n",
    "print(list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 712\n"
     ]
    }
   ],
   "source": [
    "# get the number d of features of each sample\n",
    "d = X_categorical.shape[1]\n",
    "\n",
    "# get the number m of samples\n",
    "m = X_categorical.shape[0]\n",
    "\n",
    "#let's see what the number of samples is\n",
    "print(\"Number of samples: {}\".format(m))\n",
    "\n",
    "#now encode categorical variables using integers and one-hot-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "onehot_encoder = OneHotEncoder()\n",
    "\n",
    "# encode the first column of the data matrix into indicator variables\n",
    "\n",
    "X_tmp = label_encoder.fit_transform(X_categorical[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guida/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_tmp = X_tmp.reshape(X_tmp.shape[0],1)\n",
    "X = onehot_encoder.fit_transform(X_tmp[:,0].reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guida/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/guida/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/guida/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/guida/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# repeat for the other categorical input variables, which have indices 1, 3, 4, and 6 in the X_categorical\n",
    "\n",
    "index_categorical = [1,3,4,6]\n",
    "\n",
    "for i in range(1,7):\n",
    "    if i in index_categorical:\n",
    "        X_tmp = label_encoder.fit_transform(X_categorical[:,i])\n",
    "        X_tmp = X_tmp.reshape(X_tmp.shape[0],1)\n",
    "        X_tmp = onehot_encoder.fit_transform(X_tmp[:,0].reshape(-1,1)).toarray()\n",
    "        X = np.hstack((X,X_tmp))\n",
    "    else:\n",
    "        X_tmp = X_categorical[:,i]\n",
    "        X_tmp = X_tmp.reshape(X_tmp.shape[0],1)\n",
    "        X = np.hstack((X,X_tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The class labels are already 0-1, so we can use them directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "# properly encode the target labels\n",
    "Y = label_encoder.fit_transform(Y)\n",
    "K = max(Y) + 1 # number of classes\n",
    "\n",
    "print(\"Number of classes: \"+str(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $m$ total data points, keep $m\\_training = 70$ data points as data for ***training and validation*** and $m\\_test = m - m\\_training$ as test data. Splitting is random, using as seed your ID number. Make sure that the training set contains at least 10 instances from each class.If it does not, modify the code so to apply a random\n",
    "permutation (or the same permutation multiple times) to the samples until this happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0\n",
      " 1 1 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 0 1]\n",
      "0.4044943820224719\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and validation data\n",
    "\n",
    "# load a package which is useful for the training-test splitting\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# number of samples\n",
    "m = np.shape(X)[0]\n",
    "\n",
    "#Divide in training and test: make sure that your training set\n",
    "#contains at least 10 elements from class 1 and at least 10 elements\n",
    "#from class -1! If it does not, modify the code so to apply more random\n",
    "#permutations (or the same permutation multiple times) until this happens.\n",
    "\n",
    "permutation = np.random.permutation(m)\n",
    "X = X[permutation]\n",
    "Y = Y[permutation]\n",
    "\n",
    "m_training = 70  # use 70 samples for training + validation...\n",
    "m_test = m-m_training # and the rest for testing\n",
    "\n",
    "# test_size is the proportion of samples in the test set\n",
    "X_training, X_test, Y_training, Y_test = train_test_split(X, Y, test_size =float(m_test)/float(m), random_state = IDnumber)\n",
    "\n",
    "print(Y_training)\n",
    "\n",
    "m_training = X_training.shape[0]\n",
    "m_test = X_test.shape[0]\n",
    "\n",
    "#let's see what the fraction of ones in the entire dataset is\n",
    "print(float(sum(Y_training)+sum(Y_test))/float(m_training+m_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the data to have zero-mean and unit variance (columnwise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the Features Matrix\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X = X.astype(np.float64) #standard scaler works with double precision data\n",
    "X_training = X_training.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "\n",
    "#let's use the standard scaling; we degine the scaling for the entire dataset\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "#let's apply the scaling to the training set\n",
    "\n",
    "X_training = scaler.transform(X_training)\n",
    "#let's apply the scaling to the test set\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Logistic Regression\n",
    "\n",
    "We now perform logistic regression using the function provided by Scikit-learn.\n",
    "\n",
    "Note: as provided by Scikit-learn, logistic regression is always implemented using regularization. However, the impact of regularization can be dampened to have almost no regularization by changing the parameter $C$, which is the inverse of $\\lambda$. Therefore to have no regularization, which is $\\lambda = 0$ for the model seen in class, we need $C$ to have a large value. Here we fix $C = 100000000$.\n",
    "\n",
    "[Note that the intercept is estimated in the model.]\n",
    "\n",
    "For all our models we are going to use 10-fold cross validation to estimate the parameters (when needed) and/or estimate the validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values of parameter C tried in 10-fold Cross-Validation: [100000000]\n",
      "Accuracies obtained for the different values of C with 10-fold Cross-Validation: [0.78809524]\n",
      "Best value of parameter C according to 10-fold Cross-Validation: 100000000\n",
      "10-fold Cross-Validation accuracies obtained with the best value of parameter C: 0.7880952380952382\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# define a logistic regression model with very high C parameter -> low impact from regularization;\n",
    "# there are many solvers available to obtain the solution to the logistic regression problem, we just pick\n",
    "# one of them; 'cv' is the number of folds in cross-validation; we also specify l2 as regularization penalty,\n",
    "# just to pick one; Cs contains the values of C to be tested and to pick from with validation. Here we\n",
    "# are interested in only 1 value of C, and use cross-validation just to estimate the validation error\n",
    "# in a same way as other models\n",
    "\n",
    "reg = linear_model.LogisticRegressionCV(Cs=[100000000], solver='newton-cg',cv=10, penalty='l2')\n",
    "\n",
    "#fit the model on training data\n",
    "reg.fit(X_training, Y_training)\n",
    "\n",
    "# the attribute 'Cs_' contains ALL the values of C evaluated in cross-validation;\n",
    "# let's print them\n",
    "print(\"Values of parameter C tried in 10-fold Cross-Validation: {}\".format( reg.Cs_ ))\n",
    "\n",
    "# the attribute 'scores_' contains the accuracy obtained in each fold, for each value \n",
    "# of C tried; we now compute the average accuracy across the 10 folds\n",
    "\n",
    "CV_accuracies = np.divide(np.sum(reg.scores_[1],axis=0),10)\n",
    "\n",
    "# let's print the average accuracies obtained for the various values of C\n",
    "\n",
    "print(\"Accuracies obtained for the different values of C with 10-fold Cross-Validation: {}\".format( CV_accuracies ))\n",
    "\n",
    "# the attribute 'C_' contains the best value of C as identified by cross-validation;\n",
    "# let's print it\n",
    "\n",
    "print(\"Best value of parameter C according to 10-fold Cross-Validation: {}\".format( reg.C_[0] ))\n",
    "\n",
    "# let's store the best CV accuracy, and then print it\n",
    "reg_best_CV_accuracy = max(CV_accuracies)\n",
    "print(\"10-fold Cross-Validation accuracies obtained with the best value of parameter C: {}\".format( reg_best_CV_accuracy ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the logistic regression function in Scikit-learn has many optional parameters. Read the documentation if you want to understand what they do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1\n",
    "### Learn the best model from Logistic Regression on the entire training set and examine coefficients (by printing and plotting them)\n",
    "\n",
    "Note that you can use simply $linear\\_model.LogisticRegression()$, that does not use cross-validation, without passing the best value of $C$ (and then fit())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients obtained using the entire training set: [[ 0.18094418  0.61787856 -0.68842006  0.48518341 -0.48518341  0.07174016\n",
      "   0.08686791  0.10621297 -0.44427488  0.21531998 -0.22868873  0.02820044\n",
      "  -0.36685128  0.32546482  0.13056472  0.02820044  0.02520543  0.02820044\n",
      "   0.0125761   3.27786641 -0.52517633  0.58069796  0.21663657]]\n",
      "Intercept: [-0.33533648]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's define the Logistic Regression model\n",
    "reg_full = linear_model.LogisticRegression(C=1e8, solver='newton-cg') \n",
    "\n",
    "# get the best model using the entire training dataset\n",
    "#ADD CODE!\n",
    "reg_full.fit(X_training, Y_training)\n",
    "\n",
    "# print the coefficients from the logistic regression model.\n",
    "print(\"Coefficients obtained using the entire training set: {}\".format( reg_full.coef_ ))\n",
    "\n",
    "# note that the intercept is not in coef_, it is in intercept_\n",
    "\n",
    "print(\"Intercept: {}\".format( reg_full.intercept_ ))\n",
    "\n",
    "# Plot the coefficients\n",
    "reg_coef = reg_full.coef_.reshape(reg_full.coef_.shape[1],)\n",
    "plt.figure()\n",
    "ind = np.arange(1,len(reg_coef)+1) \n",
    "width = 0.45\n",
    "plt.bar(np.arange(1,len(reg_coef)+1) , reg_coef, width, color='r')\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Logistic Regression Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 2\n",
    "\n",
    "### Questions: How many coefficients do you get? Why? How many of them are \"close\" to 0? (max 5 lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of coefficients are: 23\n",
      "The number of coefficients close to 0 are: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of coefficients are:\",len(ind))\n",
    "condition=0.05\n",
    "print(\"The number of coefficients close to 0 are:\",(abs(reg_coef)<0.05).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs:\n",
    "- 23 is the number of features after applying the encoders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 3\n",
    "### Predict labels on training and test\n",
    "\n",
    "- Compute the predicted labels on training and test data using reg.predict\n",
    " - Evaluate the accuracy using metrics.accuracy_score from scikit-learn (it returns the percentage of data correctly classified).\n",
    " - Evaluate the score used by logistic regression on training and test data using metrics.accuracy_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set = 0.9\n",
      "Accuracy on test set = 0.7102803738317757\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# prediction on training data\n",
    "Y_training_prediction_LR = reg_full.predict(X_training) #complete!\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn for training dataset\n",
    "#ADD CODE\n",
    "\n",
    "accuracy_training = metrics.accuracy_score(Y_training, Y_training_prediction_LR)\n",
    "print(\"Accuracy on training set =\", accuracy_training)\n",
    "\n",
    "# prediction on test data\n",
    "Y_test_prediction_LR = reg_full.predict(X_test) #complete!\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn for test dataset\n",
    "#ADD CODE\n",
    "\n",
    "accuracy_test = metrics.accuracy_score(Y_test, Y_test_prediction_LR)\n",
    "print(\"Accuracy on test set =\", accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO 4\n",
    "### Use L2 regularized logistic regression with cross-validation\n",
    "\n",
    "We perform the L2 regularization for different values of the regularization parameter $C$, and use the Scikit-learn function to perform cross-validation (CV).\n",
    "\n",
    "In L2 regularized logistic regression, the following L2 regularization term is added to the loss:\n",
    "\n",
    "$$\n",
    "    \\lambda \\sum_{i=1}^d w_i^2\n",
    "$$\n",
    "\n",
    "The parameter $C$ used by Scikit learn corresponds to the inverse of $\\lambda$, that is $C = \\frac{1}{\\lambda}$.\n",
    "\n",
    "Note: the CV in Scikit-learn is by default a *stratified* CV, that means that data is split into train-validation while maintaining the proportion of different classes in each fold.\n",
    "\n",
    "In the code below:\n",
    "- use LogisticRegressionCV() to select the best value of C with a 10-fold CV with L2 penalty;\n",
    "- use LogisticRegression() to learn the best model for the best C with L2 penalty on the entire training set\n",
    "\n",
    "Note that LogisticRegressionCV() picks some default values of C to try, but you may need to pass some other values in case for your dataset you need to explore a different interval of values. This applies every time that you use LogisticRegressionCV()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values of parameter C tried in 10-fold Cross-Validation: [1.00000000e-04 7.74263683e-04 5.99484250e-03 4.64158883e-02\n",
      " 3.59381366e-01 2.78255940e+00 2.15443469e+01 1.66810054e+02\n",
      " 1.29154967e+03 1.00000000e+04]\n",
      "Accuracies obtained for the different values of C with 10-fold Cross-Validation: [0.54285714 0.54285714 0.77202381 0.78392857 0.77380952 0.79047619\n",
      " 0.77380952 0.78809524 0.78809524 0.78809524]\n",
      "[2.7825594]\n",
      "10-fold Cross-Validation accuracies obtained with the best value of parameter C: 0.7904761904761904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=2.782559402207126, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
       "          solver='newton-cg', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define the model using LogisticRegressionCV passing an appropriate solver, cv value, and choice of penalty\n",
    "regL2 = linear_model.LogisticRegressionCV(solver='newton-cg', cv=10, penalty='l2') #complete!\n",
    "\n",
    "#fit the model on training data\n",
    "#ADD CODE\n",
    "regL2.fit(X_training, Y_training)\n",
    "# the attribute 'Cs_' contains ALL the values of C evaluated in cross-validation;\n",
    "# let's print them\n",
    "\n",
    "print(\"Values of parameter C tried in 10-fold Cross-Validation: {}\".format( regL2.Cs_ ))\n",
    "\n",
    "# the attribute 'scores_' contains the accuracy obtained in each fold, for each value \n",
    "# of C tried; we now compute the average accuracy across the 10 folds\n",
    "\n",
    "CV_accuracies = np.divide(np.sum(regL2.scores_[1],axis=0),10)#complete!\n",
    "\n",
    "# let's print the average accuracies obtained for the various values of C\n",
    "\n",
    "print(\"Accuracies obtained for the different values of C with 10-fold Cross-Validation: {}\".format( CV_accuracies ))\n",
    "\n",
    "# the attribute 'C_' contains the best value of C as identified by cross-validation;\n",
    "# let's print it\n",
    "print(regL2.C_)\n",
    "# ADD CODE\n",
    "\n",
    "# let's store the best CV accuracy, and then print it\n",
    "regL2_best_CV_accuracy = max(CV_accuracies)\n",
    "print(\"10-fold Cross-Validation accuracies obtained with the best value of parameter C: {}\".format( regL2_best_CV_accuracy ))\n",
    "\n",
    "#define the model using the best C and an appropriate solver\n",
    "\n",
    "regL2_full =  linear_model.LogisticRegression(solver='newton-cg', C=regL2.C_[0]) # Complete\n",
    "\n",
    "\n",
    "#fit the model using the best C on the entire training set\n",
    "\n",
    "# ADD CODE\n",
    "regL2_full.fit(X_training, Y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 5: Print and plot the coefficients from logistic regression with and without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients obtained using Logistic Regression: [[ 0.18094418  0.61787856 -0.68842006  0.48518341 -0.48518341  0.07174016\n",
      "   0.08686791  0.10621297 -0.44427488  0.21531998 -0.22868873  0.02820044\n",
      "  -0.36685128  0.32546482  0.13056472  0.02820044  0.02520543  0.02820044\n",
      "   0.0125761   3.27786641 -0.52517633  0.58069796  0.21663657]]\n",
      "Coefficients obtained using L2 regularized Logistic Regression: [[ 3.29365806e-01  5.78218295e-01 -7.84350359e-01  4.42252395e-01\n",
      "  -4.42252395e-01 -5.18909639e-02 -1.16805243e-01  2.52213036e-01\n",
      "  -2.97261948e-01  2.02979268e-01 -1.67197298e-01  4.11684431e-06\n",
      "  -2.35238378e-01  1.58926971e-01  1.60337994e-01  4.11684431e-06\n",
      "   3.67961613e-06  4.11684431e-06  1.83592251e-06  1.56840067e+00\n",
      "  -2.56617609e-01  4.57573789e-01  2.45461961e-02]]\n",
      "Intercept: [-0.58795841]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print the coefficients from logistic regression\n",
    "# ADD CODE\n",
    "print('Coefficients obtained using Logistic Regression: {}'.format(reg_full.coef_ ))\n",
    "\n",
    "#print the coefficients from L2 regularized logistic regression\n",
    "# ADD CODE\n",
    "print('Coefficients obtained using L2 regularized Logistic Regression: {}'.format(regL2_full.coef_ ))\n",
    "\n",
    "# note that the intercept is not in coef_, it is in intercept_\n",
    "\n",
    "print(\"Intercept: {}\".format( regL2_full.intercept_ ))\n",
    "\n",
    "# Plot the coefficients\n",
    "regL2_full_coef = regL2_full.coef_.reshape(regL2_full.coef_.shape[1],)\n",
    "ind = np.arange(1,len(reg_coef)+1)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(ind, reg_coef, width, color='r')\n",
    "rects2 = ax.bar(ind + width, regL2_full_coef, width, color='y')\n",
    "ax.legend((rects1[0], rects2[0]), ('Log Regr', 'Log Regr + L2 Regul'))\n",
    "plt.xlabel('Coefficient Idx')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Logistic Regression Coefficients: Standard and Regularized Version')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 6: how do the coefficients from L2 regularization compare to the ones from logistic regression? (max 5 lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of coefficients are: 23\n",
      "The number of coefficients close to 0 are: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of coefficients are:\",len(ind))\n",
    "condition=0.05\n",
    "print(\"The number of coefficients close to 0 are:\",(abs(regL2_full_coef)<0.05).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obs:\n",
    "- Same numbers of coefficient.\n",
    "- Generalized abs value reduction  of the coefficients.\n",
    "- One coefficient more close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 7: obtain classification accuracy on training and test data for the L2 regularized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9\n",
      "Test accuracy: 0.7414330218068536\n"
     ]
    }
   ],
   "source": [
    "#now get training and test error and print training and test accuracy\n",
    "\n",
    "# predictions on training data \n",
    "Y_training_prediction_LR_L2 = regL2_full.predict(X_training)  # Complete\n",
    "\n",
    "# predictions on test data \n",
    "Y_test_prediction_LR_L2 =  regL2_full.predict(X_test)  # Complete\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn on training data\n",
    "# ADD CODE\n",
    "print('Training accuracy:', metrics.accuracy_score(Y_training, Y_training_prediction_LR_L2))\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn on test data\n",
    "# ADD CODE\n",
    "print('Test accuracy:', metrics.accuracy_score(Y_test, Y_test_prediction_LR_L2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 8: how does accuracy compare to logistic regression? Comment (max 5 lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roughly identical values therefore very similar performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TODO 9: use larger datasets for training set\n",
    "\n",
    "Perform the same estimation procedures using different more points on the training data, that is fix $m_{training} = 500$. You can simply copy and paste all the code above into the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction in the entire dataset : 0.4044943820224719\n"
     ]
    }
   ],
   "source": [
    "# ADD CODE\n",
    "m_training = 500  # use 500 samples for training + validation...\n",
    "m_test = m-m_training # and the rest for testing\n",
    "\n",
    "# test_size is the proportion of samples in the test set\n",
    "X_training, X_test, Y_training, Y_test = train_test_split(X, Y, test_size =float(m_test)/float(m), random_state = IDnumber)\n",
    "\n",
    "m_training = X_training.shape[0]\n",
    "m_test = X_test.shape[0]\n",
    "\n",
    "#let's see what the fraction of ones in the entire dataset is\n",
    "print(\"Fraction in the entire dataset :\", float(sum(Y_training)+sum(Y_test))/float(m_training+m_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the Features Matrix\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X = X.astype(np.float64) #standard scaler works with double precision data\n",
    "X_training = X_training.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "\n",
    "#let's use the standard scaling; we degine the scaling for the entire dataset\n",
    "scaler = preprocessing.StandardScaler().fit(X)\n",
    "\n",
    "#let's apply the scaling to the training set\n",
    "\n",
    "X_training = scaler.transform(X_training)\n",
    "#let's apply the scaling to the test set\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values of parameter C tried in 10-fold Cross-Validation: [100000000]\n",
      "Accuracies obtained for the different values of C with 10-fold Cross-Validation: [0.79407123]\n",
      "Best value of parameter C according to 10-fold Cross-Validation: 100000000\n",
      "10-fold Cross-Validation accuracies obtained with the best value of parameter C: 0.7940712284913967\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# define a logistic regression model with very high C parameter -> low impact from regularization;\n",
    "# there are many solvers available to obtain the solution to the logistic regression problem, we just pick\n",
    "# one of them; 'cv' is the number of folds in cross-validation; we also specify l2 as regularization penalty,\n",
    "# just to pick one; Cs contains the values of C to be tested and to pick from with validation. Here we\n",
    "# are interested in only 1 value of C, and use cross-validation just to estimate the validation error\n",
    "# in a same way as other models\n",
    "\n",
    "reg = linear_model.LogisticRegressionCV(Cs=[100000000], solver='newton-cg',cv=10, penalty='l2')\n",
    "\n",
    "#fit the model on training data\n",
    "reg.fit(X_training, Y_training)\n",
    "\n",
    "# the attribute 'Cs_' contains ALL the values of C evaluated in cross-validation;\n",
    "# let's print them\n",
    "print(\"Values of parameter C tried in 10-fold Cross-Validation: {}\".format( reg.Cs_ ))\n",
    "\n",
    "# the attribute 'scores_' contains the accuracy obtained in each fold, for each value \n",
    "# of C tried; we now compute the average accuracy across the 10 folds\n",
    "\n",
    "CV_accuracies = np.divide(np.sum(reg.scores_[1],axis=0),10)\n",
    "\n",
    "# let's print the average accuracies obtained for the various values of C\n",
    "\n",
    "print(\"Accuracies obtained for the different values of C with 10-fold Cross-Validation: {}\".format( CV_accuracies ))\n",
    "\n",
    "# the attribute 'C_' contains the best value of C as identified by cross-validation;\n",
    "# let's print it\n",
    "\n",
    "print(\"Best value of parameter C according to 10-fold Cross-Validation: {}\".format( reg.C_[0] ))\n",
    "\n",
    "# let's store the best CV accuracy, and then print it\n",
    "reg_best_CV_accuracy = max(CV_accuracies)\n",
    "print(\"10-fold Cross-Validation accuracies obtained with the best value of parameter C: {}\".format( reg_best_CV_accuracy ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients obtained using the entire training set: [[ 0.63410819 -0.01535786 -0.54201567  0.56994916 -0.56994916 -0.62803531\n",
      "   0.28199436  0.11714507 -0.03524977 -0.25139024 -0.33104055 -1.12667217\n",
      "  -0.07469818  0.20869188  0.08955854  0.04219983 -0.94623272 -0.02769994\n",
      "   0.02586888  0.07126744  0.10957728 -0.04352781 -0.08151503]]\n",
      "Intercept: [-0.68979105]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's define the Logistic Regression model\n",
    "reg_full = linear_model.LogisticRegression(solver='newton-cg', C=100000000)#COMPLETE!\n",
    "\n",
    "# get the best model using the entire training dataset\n",
    "reg_full.fit(X_training, Y_training)\n",
    "\n",
    "# print the coefficients from the logistic regression model.\n",
    "print(\"Coefficients obtained using the entire training set: {}\".format( reg_full.coef_ ))\n",
    "\n",
    "# note that the intercept is not in coef_, it is in intercept_\n",
    "print(\"Intercept: {}\".format( reg_full.intercept_ ))\n",
    "\n",
    "# Plot the coefficients\n",
    "reg_coef = reg_full.coef_.reshape(reg_full.coef_.shape[1],)\n",
    "plt.figure()\n",
    "ind = np.arange(1,len(reg_coef)+1)  # the x locations for the groups\n",
    "width = 0.45       # the width of the bars\n",
    "plt.bar(ind, reg_coef, width, color='r')\n",
    "plt.xlabel('Coefficient Index')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Logistic Regression Coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.824\n",
      "Test accuracy: 0.7971698113207547\n",
      "\n",
      " Values of parameter C tried in 10-fold Cross-Validation: [1.00000000e-04 7.74263683e-04 5.99484250e-03 4.64158883e-02\n",
      " 3.59381366e-01 2.78255940e+00 2.15443469e+01 1.66810054e+02\n",
      " 1.29154967e+03 1.00000000e+04]\n",
      "\n",
      "  Accuracies obtained for the different values of C with 10-fold Cross-Validation: [0.59402161 0.66443778 0.80419528 0.78794878 0.78802881 0.79407123\n",
      " 0.79407123 0.79407123 0.79407123 0.79407123]\n",
      "[0.00599484]\n",
      "\n",
      " 10-fold Cross-Validation accuracies obtained with the best value of parameter C: 0.8041952781112446\n",
      "\n",
      " The number of coefficients are: 23\n",
      "\n",
      " The number of coefficients close to 0 are: 6\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# prediction on training data\n",
    "Y_training_prediction_LR = reg_full.predict(X_training)\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn for training dataset\n",
    "print('Training accuracy:', metrics.accuracy_score(Y_training, Y_training_prediction_LR))\n",
    "\n",
    "# prediction on test data\n",
    "Y_test_prediction_LR = reg_full.predict(X_test) #complete!\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn for test dataset\n",
    "print('Test accuracy:', metrics.accuracy_score(Y_test, Y_test_prediction_LR))\n",
    "\n",
    "#define the model using LogisticRegressionCV passing an appropriate solver, cv value, and choice of penalty\n",
    "regL2 = linear_model.LogisticRegressionCV(solver='newton-cg', cv=10, penalty='l2')\n",
    "\n",
    "#fit the model on training data\n",
    "regL2.fit(X_training, Y_training)\n",
    "\n",
    "# the attribute 'Cs_' contains ALL the values of C evaluated in cross-validation;\n",
    "# let's print them\n",
    "print(\"\\n Values of parameter C tried in 10-fold Cross-Validation: {}\".format( regL2.Cs_ ))\n",
    "\n",
    "# the attribute 'scores_' contains the accuracy obtained in each fold, for each value \n",
    "# of C tried; we now compute the average accuracy across the 10 folds\n",
    "CV_accuracies = np.divide(np.sum(regL2.scores_[1],axis=0),10)\n",
    "\n",
    "# let's print the average accuracies obtained for the various values of C\n",
    "print(\"\\n  Accuracies obtained for the different values of C with 10-fold Cross-Validation: {}\".format( CV_accuracies ))\n",
    "\n",
    "# the attribute 'C_' contains the best value of C as identified by cross-validation;\n",
    "# let's print it\n",
    "print(regL2.C_)\n",
    "\n",
    "# let's store the best CV accuracy, and then print it\n",
    "regL2_best_CV_accuracy = max(CV_accuracies)\n",
    "print(\"\\n 10-fold Cross-Validation accuracies obtained with the best value of parameter C: {}\".format( regL2_best_CV_accuracy ))\n",
    "print(\"\\n The number of coefficients are:\",len(ind))\n",
    "condition=0.05\n",
    "print(\"\\n The number of coefficients close to 0 are:\",(abs(reg_coef)<0.05).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients obtained using Logistic Regression: [[ 0.63410819 -0.01535786 -0.54201567  0.56994916 -0.56994916 -0.62803531\n",
      "   0.28199436  0.11714507 -0.03524977 -0.25139024 -0.33104055 -1.12667217\n",
      "  -0.07469818  0.20869188  0.08955854  0.04219983 -0.94623272 -0.02769994\n",
      "   0.02586888  0.07126744  0.10957728 -0.04352781 -0.08151503]]\n",
      "\n",
      " Coefficients obtained using L2 regularized Logistic Regression: [[ 1.87442351e-01  3.06525366e-02 -1.90406602e-01  2.76091325e-01\n",
      "  -2.76091325e-01 -9.23362382e-02  1.83177404e-02  4.10672863e-02\n",
      "  -4.56006420e-03 -6.01436306e-02 -8.15950914e-02 -6.27681209e-02\n",
      "  -7.41661417e-02  6.97629044e-02  4.88324278e-02  2.64565831e-03\n",
      "  -5.87649674e-02 -2.90679255e-02 -5.56245284e-08  1.10625429e-01\n",
      "   8.88814747e-02 -2.67178679e-02 -7.01368561e-02]]\n",
      "\n",
      " Intercept: [-0.43519937]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training accuracy: 0.808\n",
      "Test accuracy: 0.7783018867924528\n",
      "\n",
      " The number of coefficients are: 23\n",
      "The number of coefficients close to 0 are: 6\n"
     ]
    }
   ],
   "source": [
    "#define the model using the best C and an appropriate solver\n",
    "regL2_full = linear_model.LogisticRegression(solver='newton-cg', C=regL2.C_[0])\n",
    "\n",
    "#fit the model using the best C on the entire training set\n",
    "regL2_full.fit(X_training, Y_training)\n",
    "\n",
    "#print the coefficients from logistic regression\n",
    "print('Coefficients obtained using Logistic Regression: {}'.format(reg_full.coef_ ))\n",
    "\n",
    "#print the coefficients from L2 regularized logistic regression\n",
    "print('\\n Coefficients obtained using L2 regularized Logistic Regression: {}'.format(regL2_full.coef_ ))\n",
    "\n",
    "# note that the intercept is not in coef_, it is in intercept_\n",
    "\n",
    "print(\"\\n Intercept: {}\".format(regL2_full.intercept_ ))\n",
    "\n",
    "# Plot the coefficients\n",
    "regL2_full_coef = regL2_full.coef_.reshape(regL2_full.coef_.shape[1],)\n",
    "ind = np.arange(1,len(reg_coef)+1)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "rects1 = ax.bar(ind, reg_coef, width, color='r')\n",
    "rects2 = ax.bar(ind + width, regL2_full_coef, width, color='y')\n",
    "ax.legend((rects1[0], rects2[0]), ('Log Regr', 'Log Regr + L2 Regul'))\n",
    "plt.xlabel('Coefficient Idx')\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Logistic Regression Coefficients: Standard and Regularized Version')\n",
    "plt.show()\n",
    "\n",
    "#now get training and test error and print training and test accuracy\n",
    "\n",
    "# predictions on training data \n",
    "Y_training_prediction_LR_L2 = regL2_full.predict(X_training) # Complete\n",
    "\n",
    "# predictions on test data \n",
    "Y_test_prediction_LR_L2 = regL2_full.predict(X_test) # Complete\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn on training data\n",
    "print('\\n Training accuracy:', metrics.accuracy_score(Y_training, Y_training_prediction_LR_L2))\n",
    "\n",
    "# compute accuracy as suggested above using metrics.accuracy_score from scikit-learn on test data\n",
    "print('Test accuracy:', metrics.accuracy_score(Y_test, Y_test_prediction_LR_L2))\n",
    "print(\"\\n The number of coefficients are:\",len(ind))\n",
    "condition=0.05\n",
    "print(\"The number of coefficients close to 0 are:\",(abs(reg_coef)<0.05).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TODO 10: Discuss all the questions above for the larger set (max 7 lines)\n",
    "#### Questions:\n",
    "1. How many coefficients do you get? Why? How many of them are \"close\" to 0? \n",
    "2. How do the coefficients from L2 regularization compare to the ones from logistic regression?\n",
    "3. How does accuracy compare to logistic regression? \n",
    "\n",
    "#### Answers:\n",
    "1. As you can see in the prints of the previous cells the number of coefficients and coefficients close to zero with the larger training set are the same as in the case of regularized logistic regression with smaller training set.\n",
    "2. As for a smaller training set we have same numbers of coefficient and a generalized abs value reduction.\n",
    "3. The training accuracy is lower for logistic regression, while test accuracy is better with regularization. If we compare the results for the two different training sets we have that in general the training accuracies decrease while the test accuracies increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
